{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! kaggle competitions download -c tmdb-box-office-prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "# import data processing libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', 500)\n",
    "# import visualisation libs\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi'] = 150\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read files and prepare full dataset\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "full = train.append(test, ignore_index=True, verify_integrity=True, sort=False)\n",
    "\n",
    "train = full.iloc[:3000]\n",
    "test = full.iloc[3000:]\n",
    "\n",
    "assert(train.shape[1] == test.shape[1])\n",
    "assert(full.shape == (train.shape[0] + test.shape[0], train.shape[1]))\n",
    "\n",
    "print('train', train.shape)\n",
    "print('test', test.shape)\n",
    "print('full', full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = train.select_dtypes(include=np.number).columns.tolist()\n",
    "string_cols = train.select_dtypes(include=np.object).columns.tolist()\n",
    "\n",
    "print(numeric_cols)\n",
    "print('\\n')\n",
    "print(string_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterpath base url http://image.tmdb.org/t/p/original/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_collection_name(obj):\n",
    "    import numpy\n",
    "    return list(map(str.strip, obj.split(\",\")))[1][9:-12] if isinstance(obj, str) else np.nan\n",
    "\n",
    "def _jsonify(obj):\n",
    "    import json\n",
    "    from json import JSONDecodeError\n",
    "    import numpy as np\n",
    "    import re\n",
    "    \n",
    "    if isinstance(obj, str):\n",
    "        obj = re.sub('(?<=[a-z])\\'(?=[A-z])', '', obj)\n",
    "        obj = obj.replace(\"Donners'\", \"Donners\") \\\n",
    "                      .replace(\"O'Connor Brothers\", \"O Connor Brothers\") \\\n",
    "                      .replace(\"d'Azur\", \"d Azur\") \\\n",
    "                      .replace(\"l'Audiovisuel\", \"l Audiovisuel\") \\\n",
    "                      .replace(\"Mel's\", \"Mel s\") \\\n",
    "                      .replace(\"d'Animation\", \"d Animation\") \\\n",
    "                      .replace(\"Loew's\", \"Loews\") \\\n",
    "                      .replace('\"Tor\"', 'Tor') \\\n",
    "                      .replace(\"L'image\", \"L image\") \\\n",
    "                      .replace(\"d'Aosta\", \"d Aosta\") \\\n",
    "                      .replace(\"I'm\", \"Im\") \\\n",
    "                      .replace(\"d'Ici\", \"d Ici\") \\\n",
    "                      .replace('\"DIA\"', 'DIA') \\\n",
    "                      .replace(\"Kids'\", \"Kids\") \\\n",
    "                      .replace(\"l'Eure\", \"l Eure\") \\\n",
    "                      .replace(\"It's\", \"Its\") \\\n",
    "                      .replace(\"Isn't\", \"Isnt\") \\\n",
    "                      .replace('\"Tsar\"', 'Tsar') \\\n",
    "                      .replace(\"D'Antoni\", \"D Antoni\") \\\n",
    "                      .replace(\"Gettin'\", \"Gettin\") \\\n",
    "                      .replace(\"L'image\", \"L image\") \\\n",
    "                      .replace(\"L'Aide\", \"L Aide\") \\\n",
    "                      .replace(\"Hell's\", \"Hells\") \\\n",
    "                      .replace(\"Bull's\", \"Bulls\") \\\n",
    "                      .replace(\"Cooper's\", \"Coopers\") \\\n",
    "                      .replace(\"Children's\", \"Childrens\") \\\n",
    "                      .replace(\"Whitaker's\", \"Whitakers\") \\\n",
    "                      .replace(\"Jing's\", \"Jings\") \\\n",
    "                      .replace(\"D'Artagnan\", \"D Artagnan\") \\\n",
    "                      .replace(\"L'Alma\", \"L Alma\") \\\n",
    "                      .replace(\"l'Egalité\", \"l Egalité\") \\\n",
    "                      .replace(\"Performers'\", \"Performers\") \\\n",
    "                      .replace(\"Butcher's\", \"Butchers\") \\\n",
    "                      .replace(\"Large's\", \"Larges\") \\\n",
    "                      .replace(\"We're\", \"Were\") \\\n",
    "                      .replace(\"d'Investissement\", \"d Investissement\") \\\n",
    "                      .replace(\"d'Or\", \"d Or\") \\\n",
    "                      .replace(\"Po'\", \"Po\") \\\n",
    "                      .replace(\"O' Salvation\", \"O Salvation\") \\\n",
    "                      .replace(\"Cast N'\", \"Cast N\") \\\n",
    "                      .replace(\"L'Institut\", \"L Institut\") \\\n",
    "                      .replace(\"Project '98\", \"Project 98\") \\\n",
    "                      .replace(\"Ninjas Runnin' \", \"Ninjas Runnin \") \\\n",
    "                      .replace('\"Kvadrat\"', 'Kvadrat') \\\n",
    "                      .replace(\"l'Europe\", \"l Europe\") \\\n",
    "                      .replace(\"l'Audiovisuel\", \"l Audiovisuel\") \\\n",
    "                      .replace(\"Quat'sous\", \"Quat sous\") \\\n",
    "                      .replace(\"l'Amour\", \"l Amour\") \\\n",
    "                      .replace(\"l'Image\", \"l Image\") \\\n",
    "                      .replace(\"Betsy's\", \"Betsys\") \\\n",
    "                      .replace(\"l'Audiovisuel\", \"l Audiovisuel\") \\\n",
    "                      .replace(\"Winter's\", \"Winters\") \\\n",
    "                      .replace(\"That's\", \"Thats\") \\\n",
    "                      .replace(\"d'Etat\", \"d Etat\") \\\n",
    "                      .replace(\"Devil's\", \"Devils\") \\\n",
    "                      .replace(\"Willie's\", \"Willies\") \\\n",
    "                      .replace(\"Cote D'Ivoire\", \"Cote D Ivoire\") \\\n",
    "                      .replace(\"mama's\", \"mamas\") \\\n",
    "                      .replace(\"girls' \", \"girls\") \\\n",
    "                      .replace(\"rock 'n' roll\", \"rock n roll\") \\\n",
    "                      .replace(\"artists' \", \"artists\") \\\n",
    "                      .replace(\"workers' \", \"workers\") \\\n",
    "                      .replace(\"students' \", \"students\") \\\n",
    "                      .replace(\"ladies' \", \"ladies\") \\\n",
    "                      .replace(\"years' \", \"years\") \\\n",
    "                      .replace(\"boys' \", \"boys\") \\\n",
    "                      .replace(\"'\", '\"') \\\n",
    "                      .replace(\"\\\\xa0\", \"\")\n",
    "            \n",
    "        try:\n",
    "            return json.loads(obj)\n",
    "        except JSONDecodeError:\n",
    "            return obj\n",
    "        \n",
    "    else: return np.nan\n",
    "    \n",
    "    \n",
    "def _unnest_feature(df, col, key):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    features_list = [_jsonify(obj) for obj in df[col].values]\n",
    "    flat_features_list = []\n",
    "\n",
    "    for inner_list in features_list:\n",
    "        if isinstance(inner_list, list):\n",
    "            features = {}\n",
    "            for i, dictionary in enumerate(inner_list):\n",
    "                features[col+'_{}'.format(i)] = dictionary[key]\n",
    "            features_df = pd.DataFrame.from_dict(features, orient='index').T\n",
    "            flat_features_list.append(features_df)\n",
    "\n",
    "        elif np.isnan(inner_list):\n",
    "            #print(\"missing\")\n",
    "            features = {}\n",
    "            features[col+'_0'] = np.nan\n",
    "            features_df = pd.DataFrame.from_dict(features, orient='index').T\n",
    "            flat_features_list.append(features_df)\n",
    "    \n",
    "    features_df = pd.concat([frame for frame in flat_features_list], axis='index', ignore_index=True, sort=False)\n",
    "    \n",
    "    assert(df.shape[0] == features_df.shape[0])\n",
    "    return pd.concat([df, features_df], axis='columns')\n",
    "\n",
    "def _get_cast(obj):\n",
    "    import numpy as np\n",
    "    \n",
    "    if isinstance(obj, str) and obj != \"[]\":\n",
    "        return list(filter(lambda x: ('cast_id' in x or 'character' in x or 'name' in x or 'gender' in x or 'order' in x), \\\n",
    "                           obj.split(\",\")))\n",
    "    \n",
    "    else: return np.nan\n",
    "    \n",
    "def _count_substring(listlist, substring):\n",
    "    \"\"\"\n",
    "    Count occurance of substring in string.\n",
    "\n",
    "    :param listlist: list of lists. type list.\n",
    "    :returns: counts_list. typelist. each inner_list containing a sum of substring.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    counts_list = []\n",
    "    for list in listlist:\n",
    "        counts = [string.count(substring) for string in list]\n",
    "        counts_list.append(np.sum(counts))\n",
    "    \n",
    "    return np.array(counts_list)\n",
    "\n",
    "def _get_lead_cast_feats(df, casts):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    names = []\n",
    "    genders = []\n",
    "    for cast in casts:\n",
    "        if len(cast) > 0:\n",
    "            top5_cast_names = list(filter(lambda x: 'name' in x, cast))[:5]        \n",
    "            leading_cast_names = [name[10: -1] for name in top5_cast_names]\n",
    "            leading_cast_names_df = pd.DataFrame(leading_cast_names).T.rename({0:'lead1', 1:'lead2', 2:'lead3', 3:'lead4', 4:'lead5'}, axis='columns')\n",
    "            names.append(leading_cast_names_df)\n",
    "\n",
    "            top5_cast_gender = list(map(_map_gender, list(filter(lambda x: 'gender' in x, cast))[:5]))\n",
    "            leading_cast_genders_df = pd.DataFrame(top5_cast_gender).T.rename({0:'lead1_gender', 1:'lead2_gender', 2:'lead3_gender', 3:'lead4_gender', 4:'lead5_gender'}, axis='columns')\n",
    "            genders.append(leading_cast_genders_df)\n",
    "        else:\n",
    "            # cast missing\n",
    "            names.append(pd.DataFrame([np.nan,np.nan,np.nan,np.nan,np.nan]).T \\\n",
    "                          .rename({0:'lead1', 1:'lead2', 2:'lead3', 3:'lead4', 4:'lead5'}, axis='columns'))\n",
    "            genders.append(pd.DataFrame([np.nan,np.nan,np.nan,np.nan,np.nan]).T \\\n",
    "                          .rename({0:'lead1_gender', 1:'lead2_gender', 2:'lead3_gender', 3:'lead4_gender', 4:'lead5_gender'}, axis='columns'))\n",
    "\n",
    "    leading_cast_names_df = pd.concat([df for df in names], axis='index', ignore_index=True, sort=False)\n",
    "    leading_cast_genders_df = pd.concat([df for df in genders], axis='index', ignore_index=True, sort=False)\n",
    "    assert(df.shape[0] == leading_cast_names_df.shape[0] == leading_cast_genders_df.shape[0])\n",
    "    \n",
    "    return pd.concat([df, leading_cast_names_df, leading_cast_genders_df], axis='columns', sort=False)\n",
    "\n",
    "def _map_gender(string):\n",
    "    import numpy as np\n",
    "    if isinstance(string, str):\n",
    "        if string.strip() == \"'gender': 1\":\n",
    "            return 'Female'\n",
    "        elif string.strip() == \"'gender': 2\":\n",
    "            return 'Male'\n",
    "        else:\n",
    "            return 'Unspecified'\n",
    "    else: return np.nan\n",
    "    \n",
    "def plot_information_content(feats):\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    information_content = feats.notna().sum()/feats.shape[0]\n",
    "    fig, ax = plt.subplots(figsize=(15,40))\n",
    "    information_content.plot('barh', ax=ax)\n",
    "    plt.tight_layout()\n",
    "\n",
    "def create_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    df.belongs_to_collection = df.belongs_to_collection.apply(lambda x: _get_collection_name(x))\n",
    "    df['is_sequel'] = np.where(df.belongs_to_collection.notna(), 1, 0)\n",
    "    \n",
    "    df = _unnest_feature(df, col='genres', key='name')\n",
    "    df['genres_cnt'] = _count_substring(full.genres.fillna(\"\").apply(lambda x: x.split(\",\")), \"'name'\")\n",
    "    \n",
    "    df['overview_len'] = np.array([len(row) for row in df.overview.fillna(\"\").values])\n",
    "    df['tagline_len'] = np.array([len(row) for row in df.tagline.fillna(\"\").values])\n",
    "    df['original_title_len'] = np.array([len(row) for row in df.original_title.fillna(\"\").values])\n",
    "    df['title_contains_The'] = [1 if 'The' in title[:3] else 0 for title in df.original_title.fillna(\"\").values]\n",
    "    df['is_rebranded'] = np.where(df.original_title != df.title, 1, 0)\n",
    "    \n",
    "    df = _unnest_feature(df, col='production_companies', key='name')\n",
    "    df['production_companies_cnt'] = _count_substring(full.production_companies.fillna(\"\").apply(lambda x: x.split(\",\")), \"'name'\")\n",
    "    df = _unnest_feature(df, col='production_countries', key='name')\n",
    "    df['production_countries_cnt'] = _count_substring(full.production_countries.fillna(\"\").apply(lambda x: x.split(\",\")), \"'name'\")\n",
    "    df = _unnest_feature(df, col='spoken_languages', key='name')\n",
    "    df['spoken_languages_cnt'] = _count_substring(full.spoken_languages.fillna(\"\").apply(lambda x: x.split(\",\")), \"'name'\")\n",
    "    df['is_foreign'] = np.where(df.original_language != \"en\", 1, 0)\n",
    "    df = _unnest_feature(df, col='Keywords', key='name')\n",
    "    df['keywords_cnt'] = _count_substring(full.Keywords.fillna(\"\").apply(lambda x: x.split(\",\")), \"'name'\")\n",
    "    \n",
    "    df.cast = df.cast.apply(lambda x: _get_cast(x))\n",
    "    casts = df.cast.fillna(\"\")\n",
    "    df['cast_size'] = _count_substring(casts, \"cast_id\")\n",
    "    df['missing_gender_cast'] = _count_substring(casts, \"'gender': 0\")\n",
    "    df['female_cast'] = _count_substring(casts, \"'gender': 1\")\n",
    "    df['male_cast'] = _count_substring(casts, \"'gender': 2\")\n",
    "    assert((df.female_cast + df.male_cast + df.missing_gender_cast == df.cast_size).all())\n",
    "    df['male_quota'] = df.male_cast/ df.cast_size\n",
    "    df['female_quota'] = 1 - df.male_quota\n",
    "    df = _get_lead_cast_feats(df, casts)\n",
    "    \n",
    "    df.release_date = pd.to_datetime(df.release_date, infer_datetime_format=True)\n",
    "    df.release_date.mask(df.release_date > '2020', np.nan, inplace=True)\n",
    "    df['year'] = df.release_date.apply(lambda x: x.date().year)\n",
    "    df['month'] = df.release_date.apply(lambda x: x.date().month)\n",
    "    df['day'] = df.release_date.apply(lambda x: x.date().day)\n",
    "    df['week'] = df.release_date.apply(lambda x: x.week)\n",
    "    df['summer_movie'] = np.select([df.month == 6, df.month == 7, df.month == 8], [1,1,1])\n",
    "    df['winter_movie'] = np.where(df.month == 12, 1, 0)\n",
    "    \n",
    "    df['budget'] = np.log1p(df.budget)\n",
    "    df['revenue'] = np.log1p(df.revenue)\n",
    "    \n",
    "    #df.sort_index(axis=1, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def prepare_df(df):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    df = df.copy()\n",
    "    drop_cols = ['homepage', 'imdb_id', 'poster_path', 'crew', 'cast', 'gender', 'overview', 'tagline', 'genres',\n",
    "                'production_companies', 'production_countries', 'spoken_languages', 'Keywords', 'title']\n",
    "    information_content = df.notna().sum()/df.shape[0]\n",
    "    low_information_content = [information_content < 0.05]\n",
    "    keep_cols = ['revenue', 'belongs_to_collection']\n",
    "    low_information_content_cols = pd.DataFrame(low_information_content[0]).rename({0:'low_info'}, axis='columns').query(\"low_info == True\").index.tolist()\n",
    "    low_information_content_cols = [col for col in low_information_content_cols if col not in keep_cols]\n",
    "    low_information_content_cols = []\n",
    "    \n",
    "    shape_before = df.shape\n",
    "    \n",
    "    for col in drop_cols + low_information_content_cols:\n",
    "        if col in df.columns.tolist():\n",
    "            df.drop(col, axis=1, inplace=True)\n",
    "            \n",
    "    print(\"Dropped {} cols\".format(shape_before[1] - df.shape[1]))\n",
    "    \n",
    "    # Type conversions\n",
    "    df.belongs_to_collection = df.belongs_to_collection.astype('category')\n",
    "    df.original_language = df.original_language.astype('category')\n",
    "    df.original_title = df.original_title.astype('category')\n",
    "    df.release_date = df.release_date.astype('category')\n",
    "    df.status = df.status.astype('category')\n",
    "    df.is_sequel = df.is_sequel.astype('category')\n",
    "\n",
    "    df.genres_0 = df.genres_0.astype('category')\n",
    "    df.genres_1 = df.genres_1.astype('category')\n",
    "    df.genres_2 = df.genres_2.astype('category')\n",
    "\n",
    "    df.title_contains_The = df.title_contains_The.astype('category')\n",
    "    df.is_rebranded = df.is_rebranded.astype('category')\n",
    "\n",
    "    df.production_companies_0 = df.production_companies_0.astype('category')\n",
    "    df.production_companies_1 = df.production_companies_1.astype('category')\n",
    "    df.production_companies_2 = df.production_companies_2.astype('category')\n",
    "    df.production_countries_0 = df.production_countries_0.astype('category')\n",
    "\n",
    "    df.spoken_languages_0 = df.spoken_languages_0.astype('category')\n",
    "    df.is_foreign = df.is_foreign.astype('category')\n",
    "\n",
    "    df.Keywords_0 = df.Keywords_0.astype('category')\n",
    "    df.Keywords_1 = df.Keywords_1.astype('category')\n",
    "    df.Keywords_2 = df.Keywords_2.astype('category')\n",
    "    df.Keywords_3 = df.Keywords_3.astype('category')\n",
    "    df.Keywords_4 = df.Keywords_4.astype('category')\n",
    "    df.Keywords_5 = df.Keywords_5.astype('category')\n",
    "    df.Keywords_6 = df.Keywords_6.astype('category')\n",
    "\n",
    "    df.lead1 = df.lead1.astype('category')\n",
    "    df.lead2 = df.lead2.astype('category')\n",
    "    df.lead3 = df.lead3.astype('category')\n",
    "    df.lead4 = df.lead4.astype('category')\n",
    "    df.lead5 = df.lead5.astype('category')\n",
    "\n",
    "    df.lead1_gender = df.lead1_gender.astype('category')\n",
    "    df.lead2_gender = df.lead2_gender.astype('category')\n",
    "    df.lead3_gender = df.lead3_gender.astype('category')\n",
    "    df.lead4_gender = df.lead4_gender.astype('category')\n",
    "    df.lead5_gender = df.lead5_gender.astype('category')\n",
    "\n",
    "    df.year = df.year.astype('category')\n",
    "    df.month = df.month.astype('category')\n",
    "    df.day = df.day.astype('category')\n",
    "    df.week = df.week.astype('category')\n",
    "    df.summer_movie = df.summer_movie.astype('category')\n",
    "    df.winter_movie = df.winter_movie.astype('category')\n",
    "    \n",
    "    shape_after = df.shape\n",
    "    assert(shape_before[0] == shape_after[0])\n",
    "    \n",
    "    df.to_parquet('feats_dropped_cols.parquet')\n",
    "    # Dummies\n",
    "    df = pd.get_dummies(df, drop_first=True)\n",
    "    col_map = {col : str(i) for col, i in zip(df.columns, range(len(df.columns)))}\n",
    "    # Save files\n",
    "    import json\n",
    "    with open('col_map.json', 'w') as f:\n",
    "        json.dump(col_map, f)\n",
    "    df.rename(col_map, axis=1, inplace=True)\n",
    "        \n",
    "    train_df = df.iloc[:3000].copy()\n",
    "    test_df = df.iloc[3000:].copy()\n",
    "    assert( (train_df.columns == test_df.columns).all() )\n",
    "    \n",
    "    #train_df.drop(col_map['id'], axis=1, inplace=True)\n",
    "    #test_df.drop(col_map['id'], axis=1, inplace=True)\n",
    "    #test_df.drop(col_map['revenue'], axis=1, inplace=True)\n",
    "    \n",
    "    train_df.to_parquet('train_df.parquet', 'pyarrow')\n",
    "    test_df.to_parquet('test_df.parquet', 'pyarrow')\n",
    "    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Crew features\n",
    "2. Bin continous features\n",
    "3. Log transform budget and revenue\n",
    "4. More count features\n",
    "5. Exploratory analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time feats = create_features(full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_information_content(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time feats_dummies = prepare_df(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_dummies.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with XGBoost\n",
    "\n",
    "1. Train a model on all features\n",
    "2. Select important features\n",
    "3. Retrain model on only important features\n",
    "4. Find optimal number of estimators by plotting learning curve\n",
    "5. Retrain model on important features with optimal number of estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_parquet('train_df.parquet')\n",
    "test = pd.read_parquet('test_df.parquet')\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "excluded = ['0', '4'] # id and revenue\n",
    "predictors = [col for col in train.columns.tolist() if col not in excluded]\n",
    "target = train['4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgbr = xgb.XGBRegressor(objective='reg:squarederror',\n",
    "                        eval_metric='rmse',\n",
    "                        n_jobs=-1,\n",
    "                        random_state=42,\n",
    "                        n_estimators=200)\n",
    "\n",
    "xgbr.fit(train[predictors], target)\n",
    "\n",
    "def rmsle(y, y0):\n",
    "    import numpy as np\n",
    "    return np.sqrt(np.mean(np.square(np.log1p(y) - np.log1p(y0))))\n",
    "\n",
    "y_train_pred = xgbr.predict(train[predictors])\n",
    "\n",
    "print('Train set error: ', rmsle(target, y_train_pred))\n",
    "\n",
    "import json\n",
    "with open('col_map.json') as f:\n",
    "    col_map = json.load(f)\n",
    "    \n",
    "predictor_names = [k for k, v in col_map.items() if v not in excluded]\n",
    "predictor_keys = [v for k, v in col_map.items() if v not in excluded]\n",
    "weights = xgbr.feature_importances_\n",
    "\n",
    "assert(len(predictor_names) == len(weights))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "feature_importances = pd.DataFrame(data=np.array([predictor_keys, weights]).T, index=predictor_names, columns=['key', 'importance'])\n",
    "feature_importances.importance = feature_importances.importance.astype(np.float64)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 20))\n",
    "feature_importances.query('importance > 0').sort_values('importance').plot(kind='barh', ax=ax)\n",
    "ax.set_title('Feature importance')\n",
    "ax.legend().set_visible(False)\n",
    "\n",
    "useful_features = feature_importances.query(\"importance > 0\").key\n",
    "useful_features_keys = useful_features.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(useful_features_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(y, y0):\n",
    "    \"\"\"\n",
    "    Compute root mean squared logarithmic error.\n",
    "    Basically a scaled version of rmse which penalizes large values.\n",
    "    \n",
    "    :params y: true label. type list or numpy array.\n",
    "    :params y0: predicted label. type list or numpy array.\n",
    "    \n",
    "    :returns: rmsle. type float.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    return np.sqrt(np.mean(np.square(np.log1p(y) - np.log1p(y0))))\n",
    "\n",
    "def rmse(y, y0):\n",
    "    \"\"\"\n",
    "    Compute root mean squared error.\n",
    "    \n",
    "    :params y: true label. type list or numpy array.\n",
    "    :params y0: predicted label. type list or numpy array.\n",
    "    \n",
    "    :returns: rmse. type float.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    return np.sqrt(np.mean(np.square(y - y0)))\n",
    "\n",
    "def xgb_cv(train,\n",
    "       predictors,\n",
    "       target,\n",
    "       test_size,\n",
    "       n_trees=10,\n",
    "       n_splits=10,\n",
    "       seed=42,\n",
    "       progressive=False,\n",
    "       debug=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Perform k-fold cross validation with xgboost \n",
    "    \n",
    "    :param train: training set. type pandas dataframe.\n",
    "    :param predictors: independent variables. X. type list of col names contained in train.\n",
    "    :param target: dependent variable. y. type pandas series or dataframe.\n",
    "    :param test_size: test size split. type float in (0,1).\n",
    "    :param n_trees: number of estimators in xgboost. type int in (1,inf).\n",
    "    :param n_splits: number of splits in k-fold. type int (1,inf).\n",
    "    :param seed: random state for reproducable random numbers. type int.\n",
    "    :param progressive: train n_trees number of models and plot learning curve progressively or\n",
    "    train just n_trees models distinctly. type bool.\n",
    "    :param debug: verbosity level. type bool.\n",
    "    \n",
    "    :returns: train_scores: training scores for each split. type list.\n",
    "    :returns: valid_scores: validaiton scores for each split. type list.\n",
    "    \"\"\"\n",
    "    import xgboost as xgb\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(train[predictors],\n",
    "                                                        target,\n",
    "                                                        test_size=test_size,\n",
    "                                                        random_state=seed)\n",
    "    \n",
    "    if debug: \n",
    "        print('X_train shape: {}'.format(X_train.shape))\n",
    "        print('X_test shape: {}'.format(X_test.shape))\n",
    "        print('y_train shape: {}'.format(y_train.shape))\n",
    "        print('y_test shape: {}'.format(y_test.shape))\n",
    "        \n",
    "    kf = KFold(n_splits=n_splits, random_state=seed)\n",
    "    train_scores = []\n",
    "    valid_scores = []\n",
    "    test_scores = []\n",
    "    \n",
    "    if progressive:\n",
    "        for trees in range(1, n_trees+1):\n",
    "            xgbr = xgb.XGBRegressor(objective='reg:squarederror',\n",
    "                        eval_metric='rmse',\n",
    "                        n_jobs=-1,\n",
    "                        n_estimators=trees,\n",
    "                        random_state=seed)\n",
    "            if debug: print(xgbr.n_estimators)\n",
    "            train_scores_intra_fold = []\n",
    "            valid_scores_intra_fold = []\n",
    "            test_scores_intra_fold = []\n",
    "            for train_idx, valid_idx in kf.split(X_train):\n",
    "                xgbr.fit(X_train.iloc[train_idx, :], y_train.iloc[train_idx])\n",
    "                train_scores_intra_fold.append(rmse(y_train.iloc[train_idx], \n",
    "                                               xgbr.predict(X_train.iloc[train_idx, :])))\n",
    "\n",
    "                valid_scores_intra_fold.append(rmse(y_train.iloc[valid_idx], \n",
    "                                               xgbr.predict(X_train.iloc[valid_idx, :])))\n",
    "                \n",
    "                test_scores_intra_fold.append(rmse(y_test,\n",
    "                                                    xgbr.predict(X_test)))\n",
    "                \n",
    "            train_scores.append(np.mean(train_scores_intra_fold))\n",
    "            valid_scores.append(np.mean(valid_scores_intra_fold))\n",
    "            test_scores.append(np.mean(test_scores_intra_fold))\n",
    "\n",
    "    else:\n",
    "        xgbr = xgb.XGBRegressor(objective='reg:squarederror',\n",
    "                        eval_metric='rmse',\n",
    "                        n_jobs=-1,\n",
    "                        n_estimators=n_trees,\n",
    "                        random_state=seed)\n",
    "        for train_idx, valid_idx in kf.split(X_train):\n",
    "            xgbr.fit(X_train.iloc[train_idx, :], y_train.iloc[train_idx])\n",
    "            train_scores.append(rmse(y_train.iloc[train_idx], \n",
    "                                      xgbr.predict(X_train.iloc[train_idx, :])))\n",
    "\n",
    "            valid_scores.append(rmse(y_train.iloc[valid_idx], \n",
    "                                      xgbr.predict(X_train.iloc[valid_idx, :])))\n",
    "\n",
    "    \n",
    "    return train_scores, valid_scores, test_scores, xgbr\n",
    "\n",
    "def plot_performance(train_scores, valid_scores, test_scores, progressive=False):\n",
    "       \n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    plt.style.use('seaborn')\n",
    "    import seaborn as sns\n",
    "    \n",
    "    if progressive:\n",
    "        figsize=(0.25*len(train_scores),5)\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        ax.set_xlabel('Number of estimators')\n",
    "    else: \n",
    "        figsize = (8,3)\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        ax.set_xlabel('Cross validation fold')\n",
    "    \n",
    "    ax.errorbar(x=range(1, len(train_scores)+1),\n",
    "                y=train_scores,\n",
    "                yerr=None,\n",
    "                linestyle='--',\n",
    "                linewidth=1,\n",
    "                #marker='o',\n",
    "                label='train {:.2f} +- {:.2f}'.format(np.mean(train_scores),\n",
    "                                                        np.std(train_scores)))\n",
    "    \n",
    "    ax.errorbar(x=range(1, len(valid_scores)+1),\n",
    "            y=valid_scores,\n",
    "            yerr=None,\n",
    "            linestyle='-',\n",
    "            linewidth=1,\n",
    "            #marker='o',\n",
    "            label='valid {:.2f} +- {:.2f}'.format(np.mean(valid_scores),\n",
    "                                                  np.std(valid_scores)))\n",
    "    \n",
    "    ax.errorbar(x=range(1, len(test_scores)+1),\n",
    "        y=test_scores,\n",
    "        yerr=None,\n",
    "        linestyle='-',\n",
    "        linewidth=1,\n",
    "        #marker='o',\n",
    "        label='test {:.2f} +- {:.2f}'.format(np.mean(test_scores),\n",
    "                                              np.std(test_scores)))\n",
    "    \n",
    "    ax.set_title('Learning Curve')\n",
    "    ax.set_xticks(range(1, len(train_scores)+1))\n",
    "    ax.legend()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores, valid_scores, test_scores, model = \\\n",
    "xgb_cv(train, useful_features_keys, target, test_size=0.3, n_trees=50, progressive=True, n_splits=5, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance(train_scores, valid_scores, test_scores, progressive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "test_size=0.001\n",
    "X_train, X_test, y_train, y_test = train_test_split(train[predictors],\n",
    "                                                    target,\n",
    "                                                    test_size=test_size,\n",
    "                                                    random_state=42)\n",
    "n_trees = 1\n",
    "xgbr = xgb.XGBRegressor(objective='reg:squarederror',\n",
    "                eval_metric='rmse',\n",
    "                n_jobs=-1,\n",
    "                n_estimators=n_trees,\n",
    "                random_state=42,\n",
    "                max_depth=5,\n",
    "                verbose=True)\n",
    "\n",
    "xgbr.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = xgbr.predict(X_train)\n",
    "y_test_pred = xgbr.predict(X_test)\n",
    "\n",
    "print('Train set error: ', rmse(y_train, y_train_pred))\n",
    "print('Test set error: ', rmse(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_pred = xgbr.predict(test[predictors])\n",
    "sample_submission['revenue'] = final_test_pred\n",
    "sample_submission['revenue'] = np.expm1(sample_submission.revenue)\n",
    "sample_submission.to_csv('submission25.csv', index=False)\n",
    "\n",
    "!kaggle competitions submit -c tmdb-box-office-prediction -f submission25.csv -m \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions submissions tmdb-box-office-prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "shap.initjs()\n",
    "explainer = shap.TreeExplainer(xgbr)\n",
    "shap_values = explainer.shap_values(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet('train_df.parquet')\n",
    "test = pd.read_parquet('test_df.parquet')\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "excluded = ['0', '4']\n",
    "predictors = [col for col in train.columns if col not in excluded]\n",
    "target = train['4']\n",
    "\n",
    "train_data = lgb.Dataset(data=train[predictors], label=target)\n",
    "test_data = lgb.Dataset(data=test[predictors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'objective':'regression',\n",
    "    'boosting':'dart',\n",
    "    'metric':'rmse',\n",
    "    'n_jobs':-1,\n",
    "    'random_state':42,\n",
    "    'num_trees':200,\n",
    "    'max_depth':3,\n",
    "    'verbosity':2\n",
    "}\n",
    "\n",
    "model = lgb.train(param, train_data)\n",
    "#lgbm_model = lgb.cv(param, train_data, nfold=5, stratified=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train set error: ', rmse(target, model.predict(train[predictors])))\n",
    "#print('Test set error: ', rmse(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_pred = model.predict(test[predictors])\n",
    "\n",
    "sample_submission['revenue'] = final_test_pred\n",
    "sample_submission['revenue'] = np.expm1(sample_submission.revenue)\n",
    "sample_submission.to_csv('submission25.csv', index=False)\n",
    "\n",
    "!kaggle competitions submit -c tmdb-box-office-prediction -f submission25.csv -m \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions submissions tmdb-box-office-prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation with Xgboost and GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "\n",
    "train = pd.read_parquet('train_df.parquet')\n",
    "test = pd.read_parquet('test_df.parquet')\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "excluded = ['0', '4']\n",
    "predictors = [col for col in train.columns if col not in excluded]\n",
    "target = train['4']\n",
    "\n",
    "params = {\n",
    "        'ntrees': [25, 50, 75, 100, 125, 130, 135, 140, 145, 150, 155, 175, 200, 250, 300],\n",
    "        'max_depth' : [3, 4, 5, 8, 7, 10, 9, 11, 13, 15, 17, 19, 21]\n",
    "        }\n",
    "\n",
    "\n",
    "xgbr = xgb.XGBRegressor(objective='reg:squarederror',\n",
    "                eval_metric='rmse',\n",
    "                n_jobs=-1,\n",
    "                random_state=42,\n",
    "                verbose=True)\n",
    "\n",
    "clf = GridSearchCV(xgbr,\n",
    "                   params,\n",
    "                   cv=3,\n",
    "                   n_jobs=1,\n",
    "                   verbose=3)\n",
    "\n",
    "clf.fit(train[predictors], target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "             estimator=XGBRegressor(base_score=0.5, booster='gbtree',\n",
       "                                    colsample_bylevel=1, colsample_bynode=1,\n",
       "                                    colsample_bytree=1, eval_metric='rmse',\n",
       "                                    gamma=0, importance_type='gain',\n",
       "                                    learning_rate=0.1, max_delta_step=0,\n",
       "                                    max_depth=3, min_child_weight=1,\n",
       "                                    missing=None, n_estimators=100, n_jobs=-1,\n",
       "                                    nthread=None, objective='reg:squ...\n",
       "                                    random_state=42, reg_alpha=0, reg_lambda=1,\n",
       "                                    scale_pos_weight=1, seed=None, silent=None,\n",
       "                                    subsample=1, verbose=True, verbosity=1),\n",
       "             iid='warn', n_jobs=1,\n",
       "             param_grid={'max_depth': [3, 4, 5, 8, 7, 10, 9, 11, 13, 15, 17, 19,\n",
       "                                       21],\n",
       "                         'ntrees': [25, 50, 75, 100, 125, 130, 135, 140, 145,\n",
       "                                    150, 155, 175, 200, 250, 300]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
